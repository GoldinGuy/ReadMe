{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networking\r\n",
    "\r\n",
    "This file contains all the code necessary to design, train, and test both the feature-based and Singular Value Decomposition (SVD) recommendation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import *a lot* of stuff\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import surprise\r\n",
    "import scipy\r\n",
    "import html\r\n",
    "import csv\r\n",
    "import sys\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "from scipy.sparse import csr_matrix\r\n",
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "from sklearn.utils.extmath import randomized_svd\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
    "from surprise import Reader, Dataset, SVD, dump, accuracy\r\n",
    "from utils.xml_to_dict import dict_from_xml_file\r\n",
    "\r\n",
    "sys.path.append('./utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<function dict.items>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create mapping mechanism\r\n",
    "gr_to_id = {}\r\n",
    "with open('./data/goodbooks-10k/books.csv', \"r\", encoding='utf8') as f:\r\n",
    "    reader = csv.reader(f, delimiter=\",\")\r\n",
    "    for i, line in enumerate(reader):\r\n",
    "        gr_to_id[line[1]] = line[0]\r\n",
    "\r\n",
    "gr_to_id.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<function dict.items>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare tags feature\r\n",
    "undesireables = {\r\n",
    "    'to-read',\r\n",
    "    'currently-reading',\r\n",
    "    'books-i-own',\r\n",
    "    'on-hold',\r\n",
    "    'favorite',\r\n",
    "    'favorites',\r\n",
    "    'owned',\r\n",
    "    'owned-books',\r\n",
    "    'read',\r\n",
    "    'favourites',\r\n",
    "    'default',\r\n",
    "    'kindle',\r\n",
    "    'my-books',\r\n",
    "    'to-buy',\r\n",
    "    'all-time-favorites',\r\n",
    "    're-read',\r\n",
    "    'i-own',\r\n",
    "    'ebook'\r\n",
    "}\r\n",
    "\r\n",
    "tag_defs = {}\r\n",
    "with open('./data/goodbooks-10k/tags.csv', \"r\", encoding='utf8') as f:\r\n",
    "    reader = csv.reader(f, delimiter=\",\")\r\n",
    "    for i, line in enumerate(reader):\r\n",
    "        tag_defs[line[0]] = line[1]\r\n",
    "\r\n",
    "    book_tags = {}\r\n",
    "    with open('./data/goodbooks-10k/book_tags_with_bookid.csv', \"r\", encoding='utf8') as f:\r\n",
    "        reader = csv.reader(f, delimiter=\",\")\r\n",
    "        for i, line in enumerate(reader):\r\n",
    "            gr_id = line[1]\r\n",
    "            tag_id = line[2]\r\n",
    "            count = line[3]\r\n",
    "            if gr_id not in book_tags:\r\n",
    "                book_tags[gr_id] = {}\r\n",
    "\r\n",
    "            tag_name = tag_defs[tag_id]\r\n",
    "            if tag_name not in undesireables:\r\n",
    "                book_tags[gr_id][tag_name] = count\r\n",
    "\r\n",
    "book_tags.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util to format string in easily readable way\r\n",
    "def clean_string(s):\r\n",
    "    if not s:\r\n",
    "        return s\r\n",
    "\r\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\r\n",
    "    s = html.unescape(s)\r\n",
    "    s = TAG_RE.sub('', s)\r\n",
    "    # s = s.lower()\r\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 1 to 10000\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            10000 non-null  object\n",
      " 1   image_url        10000 non-null  object\n",
      " 2   url              10000 non-null  object\n",
      " 3   author           10000 non-null  object\n",
      " 4   description      10000 non-null  object\n",
      " 5   popular_shelves  10000 non-null  object\n",
      " 6   tags             10000 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 625.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# create additional features for the content-based matrix\r\n",
    "books = []\r\n",
    "\r\n",
    "for file in os.listdir('./data/goodbooks-10k/books_xml/books_xml'):\r\n",
    "    raw, popular_shelves = dict_from_xml_file(\r\n",
    "        './data/goodbooks-10k/books_xml/books_xml/' + os.fsdecode(file))\r\n",
    "\r\n",
    "    book = {}\r\n",
    "    goodreads_id = raw['book']['id']\r\n",
    "    book['id'] = gr_to_id[goodreads_id]\r\n",
    "    book['title'] = raw['book']['title']\r\n",
    "    book['image_url'] = raw['book']['image_url']\r\n",
    "    book['url'] = raw['book']['url']\r\n",
    "    book['author'] = raw['book']['authors']['author']\r\n",
    "\r\n",
    "    # only use first author\r\n",
    "    if isinstance(book['author'], dict):\r\n",
    "        book['author'] = book['author']['name']\r\n",
    "    else:\r\n",
    "        book['author'] = book['author'][0]['name']\r\n",
    "\r\n",
    "    book['description'] = raw['book']['description']\r\n",
    "    book['description'] = clean_string(book['description'])\r\n",
    "\r\n",
    "    # add pop shelves & tags features\r\n",
    "    book['popular_shelves'] = ''\r\n",
    "    normalizing_value = 5\r\n",
    "    for key, value in popular_shelves.items():\r\n",
    "        for i in range(int(value) // normalizing_value):\r\n",
    "            book['popular_shelves'] += ' ' + key\r\n",
    "\r\n",
    "    book['tags'] = ''\r\n",
    "    tags = book_tags[goodreads_id]\r\n",
    "    for key, value in tags.items():\r\n",
    "        for i in range(int(value) // normalizing_value):\r\n",
    "            book['tags'] += ' ' + key\r\n",
    "\r\n",
    "    books.append(book)\r\n",
    "\r\n",
    "books_df = pd.DataFrame(books)\r\n",
    "books_df['id'] = books_df['id'].astype(int)\r\n",
    "books_df = books_df.sort_values(by=['id'])\r\n",
    "books_df = books_df.set_index('id')\r\n",
    "\r\n",
    "books_df['description'] = books_df['description'].fillna('')\r\n",
    "books_df.to_pickle('pickled/books.pkl')\r\n",
    "\r\n",
    "books_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(210612, 10000)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch combined dataset\r\n",
    "combined_ratings = scipy.sparse.load_npz('models/ratings_combined.npz')\r\n",
    "combined_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to reduce a matrix into its components using SVD\r\n",
    "def reduce_matrix(X, n_components=1000, n_iter=7, random_state=None):\r\n",
    "    \"\"\"\r\n",
    "    Props:\r\n",
    "        X:              matrix to reduce\r\n",
    "        n_components:   num singular values to limit to\r\n",
    "        n_iter:         num iterations for SVD\r\n",
    "        random_state:   random initial state SVD\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        U: row representations\r\n",
    "        S: singular values\r\n",
    "        V: column representations\r\n",
    "    \"\"\"\r\n",
    "    svd = TruncatedSVD(n_components=n_components, n_iter=n_iter, random_state=random_state)\r\n",
    "    reduced_matrix = svd.fit_transform(X)\r\n",
    "    return reduced_matrix, svd.singular_values_, svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature matrix\r\n",
    "books = pd.read_pickle('pickled/books.pkl')\r\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 11245)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit feature matrix on features\r\n",
    "tfidf_m_description = vectorizer.fit_transform(books['description'])\r\n",
    "print(tfidf_m_description.shape)\r\n",
    "\r\n",
    "tfidf_m_shelves = vectorizer.fit_transform(books['popular_shelves'])\r\n",
    "print(tfidf_m_shelves.shape)\r\n",
    "\r\n",
    "tfidf_m_tags = vectorizer.fit_transform(books['tags'])\r\n",
    "print(tfidf_m_tags.shape)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight smaller matrices by ratio to largest column matrix\r\n",
    "shelves_weight = tfidf_m_description.shape[1] / tfidf_m_shelves.shape[1]\r\n",
    "tags_weight = tfidf_m_description.shape[1] / tfidf_m_tags.shape[1]\r\n",
    "\r\n",
    "tfidf_m_shelves = tfidf_m_shelves.multiply(shelves_weight)\r\n",
    "tfidf_m_tags = tfidf_m_tags.multiply(tags_weight)\r\n",
    "\r\n",
    "feature_m = scipy.sparse.hstack([tfidf_m_description, tfidf_m_shelves, tfidf_m_tags])\r\n",
    "\r\n",
    "scipy.sparse.save_npz('models/feature_m', feature_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "567667.5952636951"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD on full features to calculate sum of eigen values (special set of scalars associated with a linear system of equation)\r\n",
    "U, E, V = reduce_matrix(feature_m, n_components=3000)\r\n",
    "\r\n",
    "total_eigen_values = 0\r\n",
    "for e in E:\r\n",
    "    total_eigen_values += (e*e)\r\n",
    "total_eigen_values\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "554451.312193875"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_U, E_reduced, _ = reduce_matrix(feature_m, n_components=1000)\r\n",
    "\r\n",
    "total_eigen_values = 567667.43158802704\r\n",
    "reduced_eigen_values = 0\r\n",
    "for e in E_reduced:\r\n",
    "    reduced_eigen_values += (e*e)\r\n",
    "reduced_eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reduced feature matrix\r\n",
    "np.save('models/feature_matrix_1000.npy', features_U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd matrix\r\n",
    "ratings = pd.read_pickle('pickled/ratings.pkl')\r\n",
    "\r\n",
    "reader = Reader(rating_scale=(1, 5))\r\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'book_id', 'rating']], reader)\r\n",
    "\r\n",
    "svd = SVD(n_epochs=100, n_factors=300, lr_all=0.005, reg_all=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x2088223e908>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit SVD model with training data\r\n",
    "train = data.build_full_trainset()\r\n",
    "svd.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 300)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save svd model\r\n",
    "dump.dump('models/svd_100_300', algo=svd)\r\n",
    "# convert SVD object saved by Surprise into numpy array and save that\r\n",
    "svd = dump.load('models/svd_100_300')[1]\r\n",
    "np.save('models/svd_100_300.npy', svd.qi)\r\n",
    "svd.qi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 120)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the dual collaborative & ratings based matrix\r\n",
    "ratings_U, _, _ = reduce_matrix(combined_ratings.T, n_components=20)\r\n",
    "features_U, _, _ = reduce_matrix(feature_m, n_components=100)\r\n",
    "\r\n",
    "# normalize everything\r\n",
    "max_rating = np.max(ratings_U)\r\n",
    "ratings_U = ratings_U / max_rating\r\n",
    "\r\n",
    "max_feature = np.max(features_U)\r\n",
    "features_U = features_U / max_feature\r\n",
    "\r\n",
    "recs_m = np.hstack((ratings_U, features_U))\r\n",
    "recs_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 1000)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in feature matrix (for future dev so you don't need to rerun everything)\r\n",
    "feature_m = np.load('models/feature_m_1000.npy')\r\n",
    "feature_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 300)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in SVD matrix (for future dev so you don't need to rerun everything)\r\n",
    "svd_m = np.load('models/svd_100_300.npy')\r\n",
    "svd_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 1300)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack arrays horizonatally\r\n",
    "recs_m = np.hstack((svd_m, feature_m))\r\n",
    "recs_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dual collaborative & content based matrix\r\n",
    "np.save('models/readme_m.npy', recs_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "name": "python372jvsc74a57bd00aa4897afac821febff936c1e315d75cb0dba5e4431c90b659ef5b82b1a63f37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "0aa4897afac821febff936c1e315d75cb0dba5e4431c90b659ef5b82b1a63f37"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}